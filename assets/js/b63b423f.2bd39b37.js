"use strict";(globalThis.webpackChunkphysical_ai_and_humanoid_robots=globalThis.webpackChunkphysical_ai_and_humanoid_robots||[]).push([[4929],{6983(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"module1/hands-on-simulated-environment","title":"Hands-on Simulated Robot Environment","description":"A hands-on exercise to explore the basics of simulated robot environments, focusing on robot perception and action in a virtual world.","source":"@site/docs/module1/04-hands-on-simulated-environment.md","sourceDirName":"module1","slug":"/module1/hands-on-simulated-environment","permalink":"/hackathon-humanoid-robotics-textbook/docs/module1/hands-on-simulated-environment","draft":false,"unlisted":false,"editUrl":"https://github.com/Umm-e-Hani02/hackathon-humanoid-robotics-textbook/tree/main/docs/module1/04-hands-on-simulated-environment.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Hands-on Simulated Robot Environment","description":"A hands-on exercise to explore the basics of simulated robot environments, focusing on robot perception and action in a virtual world.","keywords":["Simulated Robot","Hands-on","Robotics Simulation","Python","Perception","Action","Embodied AI"]},"sidebar":"tutorialSidebar","previous":{"title":"Sensors & Humanoid Basics","permalink":"/hackathon-humanoid-robotics-textbook/docs/module1/sensors-and-humanoid-basics"},"next":{"title":"ROS 2 Basics - Nodes, Topics, and Services","permalink":"/hackathon-humanoid-robotics-textbook/docs/module2/ros2-basics"}}');var r=i(4848),t=i(8453);const s={sidebar_position:4,title:"Hands-on Simulated Robot Environment",description:"A hands-on exercise to explore the basics of simulated robot environments, focusing on robot perception and action in a virtual world.",keywords:["Simulated Robot","Hands-on","Robotics Simulation","Python","Perception","Action","Embodied AI"]},a="Hands-on: Explore a Basic Simulated Robot Environment",l={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Setup: Installing a Minimal Simulation",id:"setup-installing-a-minimal-simulation",level:2},{value:"Exercise: Interacting with the Simulated Environment",id:"exercise-interacting-with-the-simulated-environment",level:2},{value:"Questions for Reflection",id:"questions-for-reflection",level:2},{value:"Debugging Tips",id:"debugging-tips",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"hands-on-explore-a-basic-simulated-robot-environment",children:"Hands-on: Explore a Basic Simulated Robot Environment"})}),"\n",(0,r.jsx)(n.p,{children:"This exercise will guide you through setting up and exploring a very basic simulated robot environment. The goal is to familiarize yourself with how a robot perceives its virtual surroundings and how its actions can influence that environment."}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A computer with Python installed (version 3.8 or higher)."}),"\n",(0,r.jsx)(n.li,{children:"A basic understanding of the command line."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"setup-installing-a-minimal-simulation",children:"Setup: Installing a Minimal Simulation"}),"\n",(0,r.jsx)(n.p,{children:"We'll use a simple Python library for this exercise to keep the focus on the concepts rather than complex installation. For more advanced simulations (Gazebo, Unity), we will cover those in later modules."}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Create a Project Directory"}),":\r\nOpen your terminal or command prompt and create a new folder for this exercise:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"mkdir robot_sim_exercise\r\ncd robot_sim_exercise\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Create a Virtual Environment"})," (Recommended):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python -m venv venv\r\n# On Windows:\r\n# .\\venv\\Scripts\\activate\r\n# On macOS/Linux:\r\n# source venv/bin/activate\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Install a Simple Simulation Library"}),":\r\nWe'll use a hypothetical ",(0,r.jsx)(n.code,{children:"simple_robot_sim"})," library for demonstration. In a real scenario, you might install ",(0,r.jsx)(n.code,{children:"pybullet"}),", ",(0,r.jsx)(n.code,{children:"gymnasium"}),", or similar. For this exercise, we will simulate its presence."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"exercise-interacting-with-the-simulated-environment",children:"Exercise: Interacting with the Simulated Environment"}),"\n",(0,r.jsxs)(n.p,{children:["Let's imagine our ",(0,r.jsx)(n.code,{children:"simple_robot_sim"})," library provides an API to create a robot and an environment."]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Create a Python File"}),":\r\nCreate a file named ",(0,r.jsx)(n.code,{children:"explore_robot.py"})," in your ",(0,r.jsx)(n.code,{children:"robot_sim_exercise"})," directory:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# explore_robot.py\r\n\r\n# --- Hypothetical simple_robot_sim library usage ---\r\n# In a real scenario, this would be imported and used.\r\n# For this exercise, we\'ll simulate the interaction.\r\n\r\nclass SimulatedRobot:\r\n    def __init__(self):\r\n        self.position = [0.0, 0.0]  # x, y coordinates\r\n        self.orientation = 0.0      # angle in radians\r\n        self.sensors = {\r\n            "front_distance": 100,  # distance to nearest object in front\r\n            "left_distance": 100,\r\n            "right_distance": 100,\r\n            "touch_sensor": False\r\n        }\r\n        print("Robot initialized at (0,0), facing 0 degrees.")\r\n\r\n    def move_forward(self, distance):\r\n        self.position[0] += distance * math.cos(self.orientation)\r\n        self.position[1] += distance * math.sin(self.orientation)\r\n        print(f"Moved forward by {distance}. New position: {self.position}")\r\n        self._update_sensors()\r\n\r\n    def turn_left(self, angle_deg):\r\n        self.orientation += math.radians(angle_deg)\r\n        print(f"Turned left by {angle_deg} degrees. New orientation: {math.degrees(self.orientation):.2f} degrees.")\r\n        self._update_sensors()\r\n\r\n    def turn_right(self, angle_deg):\r\n        self.orientation -= math.radians(angle_deg)\r\n        print(f"Turned right by {angle_deg} degrees. New orientation: {math.degrees(self.orientation):.2f} degrees.")\r\n        self._update_sensors()\r\n\r\n    def get_sensor_readings(self):\r\n        return self.sensors\r\n\r\n    def _update_sensors(self):\r\n        # In a real simulation, this would calculate distances based on environment\r\n        # For this simplified exercise, we\'ll just simulate some changes\r\n        import random\r\n        self.sensors["front_distance"] = random.randint(10, 150)\r\n        self.sensors["left_distance"] = random.randint(10, 150)\r\n        self.sensors["right_distance"] = random.randint(10, 150)\r\n        self.sensors["touch_sensor"] = random.choice([True, False, False, False]) # 25% chance of touch\r\n\r\nimport math\r\nimport time\r\n\r\ndef main():\r\n    robot = SimulatedRobot()\r\n\r\n    print("\\n--- Initial State ---")\r\n    print(f"Robot Position: {robot.position}")\r\n    print(f"Robot Orientation: {math.degrees(robot.orientation):.2f} degrees")\r\n    print(f"Sensor Readings: {robot.get_sensor_readings()}")\r\n\r\n    print("\\n--- Performing Actions ---")\r\n    robot.move_forward(5)\r\n    time.sleep(0.5)\r\n    print(f"Sensor Readings after move: {robot.get_sensor_readings()}")\r\n\r\n    robot.turn_left(45)\r\n    time.sleep(0.5)\r\n    print(f"Sensor Readings after turn: {robot.get_sensor_readings()}")\r\n\r\n    robot.move_forward(3)\r\n    time.sleep(0.5)\r\n    print(f"Sensor Readings after second move: {robot.get_sensor_readings()}")\r\n\r\n    if robot.get_sensor_readings()["touch_sensor"]:\r\n        print("\\nRobot detected a touch! Reacting by backing up.")\r\n        robot.move_forward(-1) # Move backward\r\n    else:\r\n        print("\\nNo touch detected.")\r\n\r\n    print("\\n--- Final State ---")\r\n    print(f"Robot Position: {robot.position}")\r\n    print(f"Robot Orientation: {math.degrees(robot.orientation):.2f} degrees")\r\n    print(f"Final Sensor Readings: {robot.get_sensor_readings()}")\r\n\r\nif __name__ == "__main__":\r\n    main()\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Run the Simulation"}),":\r\nSave the ",(0,r.jsx)(n.code,{children:"explore_robot.py"})," file and run it from your terminal:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python explore_robot.py\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"questions-for-reflection",children:"Questions for Reflection"}),"\n",(0,r.jsx)(n.p,{children:"After running the simulation, consider the following:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"How did the robot's position and orientation change after each action?"}),"\n",(0,r.jsx)(n.li,{children:'How did the sensor readings vary? What does this tell you about the robot\'s "perception" of its environment?'}),"\n",(0,r.jsx)(n.li,{children:"Imagine adding a visual sensor (camera). How would the robot's understanding of its environment change?"}),"\n",(0,r.jsx)(n.li,{children:"What are the limitations of this very simple simulation compared to a real-world robot?"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["This basic exercise illustrates the fundamental loop of ",(0,r.jsx)(n.strong,{children:"perception -> action -> feedback"})," that is central to embodied intelligence and Physical AI. In more advanced simulations, these concepts become much more sophisticated, involving complex physics engines, detailed sensor models, and realistic environments."]}),"\n",(0,r.jsx)(n.h2,{id:"debugging-tips",children:"Debugging Tips"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Syntax Errors"}),": If your Python script doesn't run, double-check for typos, missing colons, or incorrect indentation. Python is very sensitive to these."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Module Not Found"}),": Ensure you have activated your virtual environment before running ",(0,r.jsx)(n.code,{children:"python explore_robot.py"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unexpected Output"}),": Review the ",(0,r.jsx)(n.code,{children:"print"})," statements in the ",(0,r.jsx)(n.code,{children:"SimulatedRobot"})," class and ",(0,r.jsx)(n.code,{children:"main"})," function. Trace the robot's position, orientation, and sensor readings step-by-step to understand the flow."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Randomness"}),": Since the sensor readings are randomized, you might get different outputs each time. This is normal for simulating real-world variations."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453(e,n,i){i.d(n,{R:()=>s,x:()=>a});var o=i(6540);const r={},t=o.createContext(r);function s(e){const n=o.useContext(t);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),o.createElement(t.Provider,{value:n},e.children)}}}]);